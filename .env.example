# SmartRAG Environment Configuration
# Copy this file to .env.local and customize for your environment

# =
# PostgreSQL Database Configuration
# =

# Development database (default)
SMARTRAG_DB_HOST=
SMARTRAG_DB_PORT=
SMARTRAG_DB_NAME=
SMARTRAG_DB_USER=
SMARTRAG_DB_PASSWORD=

# Test database (for RSpec tests)
SMARTRAG_TEST_DB_HOST=
SMARTRAG_TEST_DB_PORT=
SMARTRAG_TEST_DB_NAME=
SMARTRAG_TEST_DB_USER=
SMARTRAG_TEST_DB_PASSWORD=

# =
# OpenAI API Configuration
# =

# Required: OpenAI API key for embeddings and LLM
# Get it from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Embedding API configuration
EMBEDDING_PROVIDER=
EMBEDDING_API_KEY=
EMBEDDING_ENDPOINT=
EMBEDDING_MODEL=
EMBEDDING_DIMENSIONS=

# LLM Configuration
LLM_PROVIDER=
LLM_API_KEY=
LLM_ENDPOINT=
LLM_MODEL=

# =
# Search Configuration
# =

# Default language for search (en, zh, ja, ko)
DEFAULT_LANGUAGE=

# Maximum search results
MAX_SEARCH_RESULTS=

# Hybrid search weights (0.0 to 1.0)
FULLTEXT_WEIGHT=
VECTOR_WEIGHT=

# RRF (Reciprocal Rank Fusion) parameter
RRF_K=

# Search result limits
DEFAULT_SEARCH_LIMIT=

# Tag boost weight for vector search (0.0 to 1.0)
TAG_BOOST_WEIGHT=

# Maximum distance threshold for relevant results (0.0 to 1.0)
MAX_DISTANCE_THRESHOLD=

# =
# Chinese Text Processing (pg_jieba)
# =

# Enable Chinese text segmentation
ENABLE_JIEBA=

# Optional: Path to custom dictionary
# JIEBA_DICT_PATH=

# =
# Document Chunking Configuration
# =

# Maximum characters per chunk
CHUNK_MAX_CHARS=

# Overlap between chunks (preserve context)
CHUNK_OVERLAP=

# Split by Markdown headers first
SPLIT_BY_HEADERS=

# Minimum chunk size (discard smaller chunks)
MIN_CHUNK_SIZE=

# =
# Document Processing
# =

# Maximum file size in MB
MAX_FILE_SIZE=

# Download timeout for URLs (seconds)
DOWNLOAD_TIMEOUT=

# =
# Redis Cache Configuration (Optional)
# =

# Enable search result caching
# CACHE_ENABLED=

# Redis URL for caching
# REDIS_URL=

# Cache TTL in seconds (default: 1 hour)
# CACHE_TTL=

# =
# Performance Settings
# =

# Thread pool size for parallel processing
THREAD_POOL_SIZE=

# Batch size for database operations
DB_BATCH_SIZE=

# Enable connection pooling
CONNECTION_POOLING=

# =
# Logging Configuration
# =

# Log level: debug, info, warn, error, fatal
LOG_LEVEL=

# Log format: json, plain
LOG_FORMAT=

# Optional: Log file path (leave empty for stdout)
# LOG_FILE_PATH=

# Enable query logging
ENABLE_QUERY_LOG=

# =
# LLM Generation Parameters
# =

# Temperature for LLM (0.0 to 2.0, higher =
LLM_TEMPERATURE=

# Maximum tokens to generate
LLM_MAX_TOKENS=

# LLM request timeout (seconds)
LLM_TIMEOUT=

# =
# Monitoring and Analytics
# =

# Log slow queries (over threshold)
LOG_SLOW_QUERIES=

# Slow query threshold in milliseconds
SLOW_QUERY_THRESHOLD=

# =
# Index Maintenance
# =

# Enable partitioned indexes
ENABLE_PARTITION=

# Enable auto vacuum
AUTO_VACUUM=

# Optional: Index rebuild schedule (cron format)
# INDEX_REBUILD_SCHEDULE=

# =
# Azure OpenAI Configuration (if using Azure)
# =

# AZURE_OPENAI_API_KEY=
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_DEPLOYMENT_NAME=
# AZURE_OPENAI_API_VERSION=

# =
# Local Embedding Model Configuration
# =

# If using local embeddings (e.g., via Ollama)
# EMBEDDING_PROVIDER=
# EMBEDDING_ENDPOINT=
# EMBEDDING_MODEL=
# EMBEDDING_DIMENSIONS=

# =
# Application Environment
# =

# Rack environment (development, test, production)
RACK_ENV=

# =
# Migration Control
# =

# Set to false to skip seed data
# SEED_DATA=
